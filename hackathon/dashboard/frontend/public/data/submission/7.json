{
  "id": 7,
  "submission_id": 7,
  "status": "completed",
  "created_at": "2025-08-05T06:23:37.384521",
  "updated_at": "2025-08-19 01:45:38",
  "owner_discord_id": "1302764972492722177",
  "project_name": "Scenarios",
  "discord_handle": "reality_spiral_1",
  "category": "Infrastructure",
  "description": "on this branch - https://github.com/elizaOS/eliza/tree/scenarios-cli/\n\n## Overview\n\nScenarios allow you to test ElizaOS agents in both local and sandboxed environments. Each scenario is defined in YAML and can include:\n- Environment setup\n- Mock service responses\n- Action tracking\n- Evaluation criteria\n- Final judgment rules\n\nMore details here: https://github.com/elizaOS/eliza/tree/scenarios-cli/packages/cli/src/commands/scenario/docs/README.md\n\n## Work in Progress\n\nThe scenario system is currently being expanded with several improvements:\n\n1. **ElizaOS-Specific Scenarios**\n   - Adding specialized scenarios for ElizaOS plugin-specific functionality and more complex projects\n2. **New Evaluators**\n   - Implementing `TrajectoryContainsActionEvaluator` for testing action sequences\n   - Verify specific actions occur in expected order\n   - Track complex multi-step agent behaviors\n   - Support for action parameter validation\n\n3. **Infrastructure Improvements**\n   - Optimizing database initialization\n   - Implementing dynamic plugin loading with better error handling\n   - Enhancing test isolation and cleanup\n   - Improved error reporting for failed evaluations\n   - Removing hardcoded initial plugins for better flexibility\n   - Generalizing sandbox environment support beyond E2B\n\n## Demo Videos\n\nShort\n- https://drive.google.com/file/d/19oo2V_NfKZCJHuAdcRN3c2l2iTiXiWzd/view?usp=sharing\n- https://drive.google.com/file/d/1fkKx8zphsDZpB8QrHy1F7oKnntqs6-0b/view?usp=sharing\n- https://drive.google.com/file/d/1uUhCCqjCdcCv9mQS5CQrkj-mXOO4nC3z/view?usp=sharing\n- https://drive.google.com/file/d/1OquQX7rn77iOH-njjU68k7KzjxsOtvWx/view?usp=sharing\n\nLong (12 mins)\n- https://drive.google.com/file/d/1-PDTFNQGeW0It5HSuH3Oi2hVPrWN1FmW/view?usp=sharing",
  "twitter_handle": "@reality_spiral",
  "github_url": "https://github.com/elizaOS/eliza/",
  "demo_video_url": "https://drive.google.com/file/d/1-PDTFNQGeW0It5HSuH3Oi2hVPrWN1FmW/view?usp=sharing",
  "project_image": "/media/projects/scenarios_project.png",
  "problem_solved": "The **ElizaOS Scenario Testing Guide** introduces a structured and extensible **scenario testing system** for ElizaOS agents. Here\u2019s a breakdown of **what problem this system solves** and **why it matters**:\n\n---\n\n## \ud83e\udde9 **Core Problem It Solves**\n\n### \u2192 **Reliable and Repeatable Testing of Autonomous Agents**\n\nElizaOS agents operate in complex environments (local, sandboxed, mocked services, or LLM-judged), often using plugins, service calls, or multi-step workflows. Testing these agents has traditionally been:\n\n* **Manual** (time-consuming, brittle)\n* **Unstructured** (prone to false positives/negatives)\n* **Non-reproducible** (sensitive to external changes like APIs, services, or LLM responses)\n\nThe scenario system addresses this by:\n\n* Providing **YAML-defined test cases** with explicit structure\n* Allowing **action-level validation**, **mocked service responses**, and **judgment criteria**\n* Enabling both **local** and **sandboxed (E2B)** testing environments\n* Supporting **automated evaluation** using internal logic or LLM-based judgment\n\n---\n\n## \ud83d\udee0\ufe0f **Key Use Cases / Benefits**\n\n### 1. **Regression Testing**\n\nEnsures previously working behavior continues to function as new features or fixes are added.\n\n### 2. **Plugin Behavior Validation**\n\nTests plugin integrations (e.g. file creation, database access) in either local or sandboxed setups with precise control over mocks and environment state.\n\n### 3. **LLM Output Validation**\n\nUses evaluators (including LLM-based) to verify whether agent responses meet expectations \u2014 important for non-deterministic LLM outputs.\n\n### 4. **Multi-Step Interaction Testing**\n\nSupports trajectory and sequence evaluators to check whether agents take the right actions in order over a conversation or task sequence.\n\n### 5. **CI/CD Integration**\n\nCan be run via CLI (`bun run test:scenarios`) for automated testing pipelines, with future support for a globally available `elizaos` command.\n\n---\n\n## \ud83d\udd0d Why This is Important in Agent Development\n\n* **Agent complexity is growing**: ElizaOS agents don't just generate text\u2014they act across systems. This makes their behavior harder to predict and test.\n* **Multi-agent ecosystems**: As more plugins and services are added, integration bugs and action failures become more likely.\n* **Judgment is often subjective**: LLM evaluators provide a layer of qualitative validation without requiring brittle string matching.\n* **Mocking is essential**: To test safely and at scale, the ability to intercept and fake service calls is critical.\n\n---\n\n## \ud83e\uddea **Comparison: Before vs After**\n\n| Aspect              | Before Scenario System               | After Scenario System                         |\n| ------------------- | ------------------------------------ | --------------------------------------------- |\n| Testing Method      | Manual CLI testing, informal scripts | YAML-defined, reproducible scenarios          |\n| Service Integration | Hard to mock or simulate             | Fully mockable with configurable responses    |\n| Evaluation          | Manual spot checks or assertions     | Automated evaluators (action tracking, LLM)   |\n| Environment Setup   | Ad hoc, brittle                      | Standardized `local` and `e2b` setups         |\n| Extensibility       | Hard to extend or generalize         | New evaluators and plugin support are modular |\n\n---\n\n## \ud83e\udde0 TL;DR\n\n**The ElizaOS Scenario Testing system solves the problem of safely, efficiently, and reproducibly testing autonomous agent behavior in dynamic environments.** It combines mocks, evaluators, and CLI automation to give developers confidence that their agents are doing the right thing \u2014 across services, plugins, and multi-step tasks.",
  "favorite_part": "What excites me most about this project is that it's not just a test harness \u2014 it's a gateway to building agents that we can actually trust. We\u2019re crafting a system that lets us validate multi-step reasoning, simulate complex environments, and use mocks and LLMs to test edge cases that were previously impossible to catch. It feels like building the future of autonomous software, where we\u2019re not just hoping agents behave \u2014 we\u2019re proving it, scenario by scenario.\n\nThere\u2019s also something deeply satisfying about how composable and extensible this system is. I can define a YAML scenario in minutes, mock services, inject agent inputs, and verify outcomes with rich evaluators. It gives me fast feedback on whether the agent is behaving intelligently, responsibly, and reliably. That tight feedback loop makes iteration feel fun again \u2014 and it feels like we're laying the foundation for a true operating system for agents.\n",
  "solana_address": "iFPqs4E589mMdBSqrf2C1HLz9MmssogGyZ9pmTDny4S",
  "ethereum_address": null
}
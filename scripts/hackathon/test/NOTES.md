# Hackathon Judging System: Test Observations Log

## 1. Database Initialization

**Action:** Ran `python scripts/hackathon/create_hackathon_db.py` to create a fresh `data/hackathon.db`.

**Observation:**
- Database file was created/reset successfully.
- Schema matches the specification in `001-setup-hackathon-database.md` (tables for submissions, research, scoring, community feedback, etc.).
- No errors encountered during initialization.

## 1. Database Initialization (continued)

**Schema Details:**
- Tables created: `hackathon_submissions`, `hackathon_scores`, `community_feedback`, `hackathon_research`.
- Each table includes all required fields as specified in the plan (see create_hackathon_db.py for full schema).
- Indexes created for performance on key columns.

## 2. Submission Processing

**Action:** Ran `python scripts/hackathon/process_submissions.py --from-json data/test_hackathon_submissions.json --db-file data/hackathon.db --output ./data/hackathon --json --json-file test_output.json` to process local test data.

**Observation:**
- Script successfully loaded 2 test submissions from the JSON file.
- Both submissions were validated and inserted into the `hackathon_submissions` table in `hackathon.db`.
- Markdown files for each submission were created in `data/hackathon/` (e.g., `5747B7.md`, `620DAA.md`).
- A consolidated JSON output (`test_output.json`) was generated as expected.
- No validation errors or failures occurred during this test.

## 3. Research & Analysis

**Action:** Ran `python scripts/hackathon/hackathon_research.py --all` to perform AI and GitHub analysis for all submissions.

**Observation:**
- Script found 2 submissions to research (matching the test data).
- For each submission, GitHub repository analysis and AI research were performed.
- Research data was completed, saved, and cached for both submissions.
- No errors or failures occurred during this step.

## 3.5 Database Verification (Post-Research)

**Action:** Queried the database to verify the state of submissions and research data after the research step.

**Observation:**
- `hackathon_submissions` table contains the following entries (submission_id | project_name | category | status):
    - 5747B7 | DeFi Yield Aggregator | DeFi | researched
    - 620DAA | AI Code Assistant | AI/Agents | researched
    - (Other entries from previous tests: TEST001, TEST002)
- `hackathon_research` table contains research data for both test submissions:
    - 5747B7: github_analysis (33 chars), market_research (2 chars), technical_assessment (2 chars)
    - 620DAA: github_analysis (33 chars), market_research (823 chars), technical_assessment (2 chars)
- Research data is present for each test submission, confirming the research step wrote to the DB as expected.

## 4. Judge Scoring

**Action:** Ran `python scripts/hackathon/hackathon_manager.py --score --all` to apply AI judge scoring to all researched submissions.

**Observation:**
- Script found 2 submissions to score (matching the test data).
- For each submission, scores and comments were generated by all four AI judges (aimarc, aishaw, spartan, peepo).
- Scoring completed successfully for both submissions; no errors or failures.
- Scores and comments were stored in the database.

## 4.5 Database Verification (Post-Scoring)

**Action:** Queried the `hackathon_scores` table to verify judge scoring results for each submission.

**Observation:**
- Each test submission (5747B7, 620DAA) has four entries in the `hackathon_scores` table, one for each judge (aimarc, aishaw, spartan, peepo), all for round 1.
- Each entry includes a weighted total score and detailed comments/justifications in JSON format.
- The presence of all expected judge scores and comments confirms the scoring step wrote to the DB as expected.

## 5. Round 2 Synthesis

**Action:** Ran `python scripts/hackathon/hackathon_manager.py --synthesize --all` to combine judge scores and community feedback, generating final verdicts.

**Observation:**
- Script processed all eligible submissions (TEST002, 5747B7, 620DAA) for Round 2 synthesis.
- For each project, final verdicts were generated by all four judges, incorporating community feedback.
- Status of each processed submission was updated to `completed` in the database.
- No errors or failures occurred during this step.

## 5.5 Database Verification (Post-Round 2 Synthesis)

**Action:** Queried the `hackathon_scores` table for round 2 to verify final verdicts and scores for each submission.

**Observation:**
- Each test submission (TEST002, 5747B7, 620DAA) has four entries in the `hackathon_scores` table for round 2, one for each judge (aimarc, aishaw, peepo, spartan).
- Each entry includes a final weighted score and a detailed final verdict in the notes (JSON format), reflecting both judge and community feedback.
- The presence of all expected round 2 scores and verdicts confirms the synthesis step wrote to the DB as expected.

## 6. Episode Generation

**Action:** Ran `python scripts/hackathon/generate_episode.py --all` to generate episode JSON files for all completed projects.

**Observation:**
- Script generated episode JSON files for all test submissions (TEST002, 5747B7, 620DAA) and saved them in `episodes/hackathon/`.
- Each episode file contains the expected unified format, including all required metadata and scenes.
- No errors or failures occurred during this step.

## 7. Collaborative Observations, Ideas, and Feedback

### User: How about we split episode generation between round 1 and round 2?
- **Observation:** Currently, episode generation only reflects Round 1 (judge scores and comments), not the final results after community voting and Round 2 synthesis.
- **Idea:** Implement two episode generation modes:
  - **Round 1 Episode:** Focuses on initial judge deliberation, scoring, and feedback (pre-community voting).
  - **Round 2 Episode:** Includes community voting results, bonus points, and final judge verdicts that synthesize both judge and community perspectives.
- **Potential Benefits:**
  - Allows for a two-part episode structure (e.g., "Judges' Deliberation" and "Final Verdict").
  - Makes the show more dramatic and transparent, showing how community input can shift outcomes.
  - Enables earlier content drops (Round 1) while community voting is ongoing, then a "finale" episode after voting closes.

---

### Assistant: Would you like the episode generator to output both versions by default, or should it be a CLI flag (e.g., --round 1 or --round 2)?
- **Proposal:** Add a CLI flag to select which round to generate, or allow both in batch mode.
- **Question:** Should the Round 2 episode include a summary of how community votes changed (or confirmed) the judges' initial opinions?

---

(Continue this section with further Q&A, ideas, and feedback as we iterate on the system.)

## Database Verification Commands Used

To manually verify the database at each step, the following sqlite3 commands were used:

- To list all submissions and their status:
  ```sh
  sqlite3 data/hackathon.db "SELECT submission_id, project_name, category, status FROM hackathon_submissions;"
  ```
- To check research data for each submission:
  ```sh
  sqlite3 data/hackathon.db "SELECT submission_id, LENGTH(github_analysis), LENGTH(market_research), LENGTH(technical_assessment) FROM hackathon_research;"
  ```
- To check all judge scores for each submission (round 1):
  ```sh
  sqlite3 data/hackathon.db "SELECT submission_id, judge_name, round, weighted_total, notes FROM hackathon_scores WHERE round=1;"
  ```
- To check all judge scores and final verdicts for each submission (round 2):
  ```sh
  sqlite3 data/hackathon.db "SELECT submission_id, judge_name, round, weighted_total, notes FROM hackathon_scores WHERE round=2;"
  ```

These commands can be run at any time to inspect the state of the hackathon database and confirm the results of each pipeline phase.

---

Absolutely! Here are several ideas to improve the flow, maintainability, and developer/user experience of the hackathon judging and episode pipeline, based on our testing and review:

---

### 1. **Centralized Status Transition Utility**
- **Problem:** Status transitions (e.g., `scored` → `community-voting` → `completed`) are handled in multiple scripts.
- **Idea:** Create a utility module or class that manages all valid status transitions and logs them. This reduces the risk of typos or inconsistent states and makes it easier to add new phases.
- **Benefit:** Fewer bugs, easier to audit, and simpler to update if the workflow changes.

---

### 2. **Unified CLI Entrypoint**
- **Problem:** Each script has its own CLI, which can be confusing for new users.
- **Idea:** Create a single CLI tool (e.g., `hackathonctl`) that wraps all major actions (init DB, process submissions, research, score, post to Discord, synthesize, generate episodes, etc.).
- **Benefit:** Easier onboarding, discoverability of commands, and scripting of the full pipeline.

---

### 3. **Configurable Pipeline Steps**
- **Problem:** The pipeline is currently linear and hardcoded.
- **Idea:** Allow users to define which steps to run via a config file or CLI flags (e.g., `--skip-research`, `--only-score`).
- **Benefit:** Faster iteration for devs, easier partial reruns, and more flexible automation.

---

### 4. **Automated End-to-End Test Script**
- **Problem:** Manual testing is time-consuming and error-prone.
- **Idea:** Create a single test runner script that executes all steps, verifies DB state, and checks for expected outputs, optionally using mock data and environment variables.
- **Benefit:** Faster regression testing, easier CI integration, and more confidence in changes.

---

### 5. **Schema Versioning & Migration**
- **Problem:** DB schema changes can break old data or scripts.
- **Idea:** Add a schema version table and lightweight migration scripts (e.g., using Alembic or a simple version check).
- **Benefit:** Safe upgrades, easier collaboration, and less risk of data loss.

---

### 6. **Better Error Reporting & Logging**
- **Problem:** Some errors are only logged to console or are cryptic.
- **Idea:** Standardize error messages, add more context, and optionally log to a file or external service.
- **Benefit:** Easier debugging and support, especially for non-core contributors.

---

### 7. **Documentation & Usage Examples**
- **Problem:** The system is powerful but complex.
- **Idea:** Expand the README(s) with real-world usage examples, troubleshooting tips, and a flowchart of the pipeline.
- **Benefit:** Lower barrier to entry for new users and contributors.

---

### 8. **Web Dashboard Enhancements**
- **Problem:** The current dashboard is functional but could be more interactive.
- **Idea:** Add real-time status updates, admin controls for advancing phases, and visualizations of voting/scoring.
- **Benefit:** More engaging for organizers and transparent for participants.

---

### 9. **API Layer for Automation**
- **Problem:** Scripts interact directly with the DB.
- **Idea:** Expose a REST API (or GraphQL) for all major actions, so external tools or UIs can trigger steps and fetch results.
- **Benefit:** Decouples backend logic from CLI, enables integrations, and future-proofs the system.

---

### 10. **Plugin System for Judges/Scoring**
- **Problem:** Adding new judges or scoring logic requires code changes.
- **Idea:** Allow judge personas, weights, and scoring logic to be defined in config files or as plugins.
- **Benefit:** More flexible, easier to experiment with new judging models.

---

(Next steps and observations will be appended below as each phase is tested.)

# Robust Data Pipeline Implementation Notes

## Step 1: Central Field Manifest
- Created `scripts/hackathon/schema.py` containing the `SUBMISSION_FIELDS` list as the single source of truth for all user-submittable fields.
- Fields match the specification in the plan and exclude contact_email as instructed.
- This file will be imported by other scripts in subsequent steps.
- Success Criteria: The file exists and contains the correct list. ✅ 

## Step 2: Refactor DB & Pydantic Model to Use Manifest
- Refactored `scripts/hackathon/create_hackathon_db.py` to dynamically generate the hackathon_submissions table columns from `SUBMISSION_FIELDS`.
- Refactored `scripts/hackathon/dashboard/app.py` to dynamically generate the `SubmissionCreate` Pydantic model from `SUBMISSION_FIELDS` using `create_model`.
- Both scripts now import the manifest from `schema.py`, ensuring a single source of truth for submission fields.
- Success Criteria: The database can be created successfully, and the FastAPI server runs without errors. ✅ 

## Step 3: Build Lightweight Migration Helper
- Created `tools/field_migrate.py`, a CLI tool to add new fields to the database and append them to `SUBMISSION_FIELDS` in `schema.py`.
- The script uses argparse for CLI parsing and supports `add <field_name>`.
- Success Criteria: The command adds the column to the database and the field to the schema manifest. ✅

## Step 4: Write a Minimal Smoke Test
- Created `scripts/hackathon/test/test_smoke.py`, which runs the core pipeline scripts (DB creation, research, scoring, episode generation) on a test submission.
- The script asserts that each stage completes with exit code 0 and prints output for debugging.
- Success Criteria: The smoke test runs to completion without errors. ✅

## Step 5: Add a Pre-commit Hook for Schema Drift
- Created `scripts/check_schema.py` to compare the database schema with `SUBMISSION_FIELDS` in `schema.py`.
- Added a pre-commit hook in `.pre-commit-config.yaml` to run this check before every commit.
- The script exits with 0 if the schema is in sync and 1 if drift is detected, printing a descriptive error.
- Success Criteria: The script prevents commits if schema drift is detected. ✅

## Step 6: Run Full Pipeline Locally
- Ran the full pipeline using the smoke test script on the new feature branch.
- All stages (DB creation, research, scoring, episode generation) completed successfully with exit code 0.
- Verified that the system works end-to-end and produces the expected outputs.
- Success Criteria: The entire pipeline executes without any errors. ✅

## Refactor Step 1: Dynamic Insert for v1 Endpoint
- Refactored the /api/v1/submissions endpoint in app.py to use dynamic insert logic based on SUBMISSION_FIELDS_V1.
- All hardcoded field names for v1 are removed; the insert statement and data dict are generated from the manifest.
- This ensures the endpoint always matches the schema and is easy to maintain as fields change.

## Refactor Step 2: Dynamic GET for Submission Detail
- Refactored the GET /api/submission/{submission_id} endpoint to support both v1 and v2 tables dynamically.
- Added a version query parameter (default v1).
- The endpoint now uses the correct manifest and table for the requested version, and builds the SELECT statement dynamically.
- Only fields present in the manifest for the requested version are returned.

## Refactor Step 3: Dynamic GET for Submissions List
- Refactored the GET /api/submissions endpoint to support both v1 and v2 tables dynamically.
- Added a version query parameter (default v1).
- The endpoint now uses the correct manifest and table for the requested version, and builds the SELECT statement dynamically.
- Only fields present in the manifest for the requested version are returned for each submission.